dataset_config:
  desc: LIMA Instruct Finetunning Dataset
  train:
    dataset_path: GAIR/lima
    sub_split_size: null

tokenizer_config:
  tokenizer_name: llama2
  tokenizer_path: "meta-llama/Llama-2-7b-hf"
  add_bos_token: true
  add_eos_token: true
  special_token_kwargs:
    pad_token: "[pad]"
    additional_tokens:
      - EOT_TOKEN

# QLORA LLama finetuned
# model_config:
#   model_name: llama2-finetuned
#   model_path: /home/hmankodi/instruct_tuning/FineTune-Llama2-LIMA/TrainingLogs/checkpoint-468
#   base_model_path: meta-llama/Llama-2-7b-hf
#   force_download: false
#   device_map: cuda:0
#   bnb_config:
#     load_in_4bit: true
#     bnb_4bit_quant_type: nf4
#     bnb_4bit_compute_dtype: "float16"
#     bnb_4bit_use_double_quant: false

model_config:
  model_name: llama2-finetuned
  model_path: /home/hmankodi/instruct_tuning/FineTune-Llama2-LIMA/TrainingLogs/checkpoint-2060
  base_model_path: meta-llama/Llama-2-7b-hf
  force_download: false
  device_map: cuda:0

generation_config:
  max_length: 2048
  top_p: 0.9
  temperature: 0.7
  num_beams: 1
  top_k: null
  do_sample: True
  repetition_penalty: 1.2

generation_samples:
  - this is a good product
  - this is a bad product
  - this is a very stupid product
