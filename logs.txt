2025-02-12 01:38:41,401 - DEBUG - utils.py - Attempting to read YAML file from './configs/train_config_llama_qlora.yaml'
2025-02-12 01:38:41,405 - INFO - utils.py - YAML file './configs/train_config_llama_qlora.yaml' loaded successfully.
2025-02-12 01:38:41,405 - DEBUG - utils.py - Configuration loaded: {'dataset_config': {'desc': 'LIMA Instruct Finetunning Dataset', 'train': {'dataset_path': 'GAIR/lima', 'sub_split_size': None}}, 'tokenizer_config': {'tokenizer_name': 'llama2', 'tokenizer_path': 'meta-llama/Llama-2-7b-hf', 'add_bos_token': True, 'add_eos_token': True, 'special_token_kwargs': {'pad_token': '[pad]', 'additional_tokens': ['EOT_TOKEN']}}, 'model_config': {'model_name': 'llama2-base', 'base_model_path': 'meta-llama/Llama-2-7b-hf', 'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}}, 'lora_config': {'r': 64, 'lora_alpha': 64, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'target_modules': ['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj']}, 'sft_trainer_config': {'save_trained_model': True, 'save_strategy': 'epoch', 'save_total_limit': 2, 'run_name': 'qlora_llama2_instruct_fulltrain_1', 'report_to': 'tensorboard', 'output_dir': './TrainingLogs/', 'overwrite_output_dir': True, 'max_seq_length': 2048, 'fp16': True, 'do_train': True, 'num_train_epochs': 15, 'per_device_train_batch_size': 20, 'prediction_loss_only': False, 'packing': False, 'logging_strategy': 'steps', 'logging_steps': 1, 'max_grad_norm': 0.3, 'seed': 'random', 'warmup_ratio': 0.05, 'learning_rate': 1e-05, 'lr_scheduler_type': 'linear', 'optim': 'paged_adamw_32bit', 'adam_beta1': 0.9, 'adam_beta2': 0.95}}
2025-02-12 01:38:41,405 - INFO - utils.py - Loaded tokenizer configuration: {'tokenizer_name': 'llama2', 'tokenizer_path': 'meta-llama/Llama-2-7b-hf', 'add_bos_token': True, 'add_eos_token': True, 'special_token_kwargs': {'pad_token': '[pad]', 'additional_tokens': ['EOT_TOKEN']}}
2025-02-12 01:38:41,405 - INFO - model.py - Initializing tokenizer 'llama2' with arguments: {'tokenizer_path': 'meta-llama/Llama-2-7b-hf', 'config': {'add_bos_token': True, 'add_eos_token': True, 'special_token_kwargs': {'pad_token': '[pad]', 'additional_tokens': ['EOT_TOKEN']}}}
2025-02-12 01:38:41,926 - DEBUG - model.py - Special tokens added to the tokenizer.
2025-02-12 01:38:41,926 - INFO - utils.py - Loaded dataset configuration: {}
2025-02-12 01:38:43,419 - INFO - train.py - Loaded Train Dataset: LIMA Instruct Finetunning Dataset, Dataset Length: 1030 with sub_split_size None.
2025-02-12 01:38:43,419 - INFO - utils.py - Loaded model configuration: {'model_name': 'llama2-base', 'base_model_path': 'meta-llama/Llama-2-7b-hf', 'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}, 'pad_token_id': 32000, 'tokenizer_length': 32002}
2025-02-12 01:38:43,420 - INFO - model.py - Initializing model 'llama2-base' with arguments: {'base_model_path': 'meta-llama/Llama-2-7b-hf', 'config': {'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}, 'pad_token_id': 32000, 'tokenizer_length': 32002}}
2025-02-12 01:39:05,086 - DEBUG - model.py - Base model loaded successfully.
2025-02-12 01:39:05,086 - INFO - utils.py - Loaded trainer configuration: {'r': 64, 'lora_alpha': 64, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'target_modules': ['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj']}
2025-02-12 01:39:06,228 - INFO - utils.py - Loaded trainer configuration: {'save_trained_model': True, 'save_strategy': 'epoch', 'save_total_limit': 2, 'run_name': 'qlora_llama2_instruct_fulltrain_1', 'report_to': 'tensorboard', 'output_dir': './TrainingLogs/', 'overwrite_output_dir': True, 'max_seq_length': 2048, 'fp16': True, 'do_train': True, 'num_train_epochs': 15, 'per_device_train_batch_size': 20, 'prediction_loss_only': False, 'packing': False, 'logging_strategy': 'steps', 'logging_steps': 1, 'max_grad_norm': 0.3, 'seed': 1440481466, 'warmup_ratio': 0.05, 'learning_rate': 1e-05, 'lr_scheduler_type': 'linear', 'optim': 'paged_adamw_32bit', 'adam_beta1': 0.9, 'adam_beta2': 0.95}
2025-02-12 01:39:06,277 - WARNING - other.py - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-02-12 01:39:06,329 - INFO - train.py - Training started.
