2025-02-11 01:52:20,293 - DEBUG - utils.py - Attempting to read YAML file from 'configs/train_config_llama.yaml'
2025-02-11 01:52:20,296 - INFO - utils.py - YAML file 'configs/train_config_llama.yaml' loaded successfully.
2025-02-11 01:52:20,296 - DEBUG - utils.py - Configuration loaded: {'dataset_config': {'desc': 'LIMA Instruct Finetunning Dataset', 'train': {'dataset_path': 'GAIR/lima', 'sub_split_size': None}}, 'tokenizer_config': {'tokenizer_name': 'llama2', 'tokenizer_path': 'meta-llama/Llama-2-7b-hf'}, 'model_config': {'model_name': 'llama2-base', 'base_model_path': 'meta-llama/Llama-2-7b-hf', 'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}}, 'lora_config': {'r': 64, 'lora_alpha': 64, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'target_modules': ['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj']}, 'sft_trainer_config': {'resume_from_checkpoint': '/home/hmankodi/instruct_tuning/TrainingLogs/checkpoint-156', 'save_trained_model': True, 'save_strategy': 'epoch', 'save_total_limit': 2, 'run_name': 'llama2_instruct_fulltrain_1', 'report_to': 'tensorboard', 'output_dir': './TrainingLogs/', 'overwrite_output_dir': True, 'max_seq_length': 2048, 'fp16': True, 'do_train': True, 'num_train_epochs': 15, 'per_device_train_batch_size': 20, 'prediction_loss_only': False, 'packing': False, 'logging_strategy': 'steps', 'logging_steps': 1, 'max_grad_norm': 0.3, 'seed': 'random', 'warmup_ratio': 0.05, 'learning_rate': 1e-05, 'lr_scheduler_type': 'cosine', 'optim': 'paged_adamw_32bit'}}
2025-02-11 01:52:20,297 - INFO - utils.py - Loaded tokenizer configuration: {'tokenizer_name': 'llama2', 'tokenizer_path': 'meta-llama/Llama-2-7b-hf'}
2025-02-11 01:52:20,298 - INFO - model.py - Initializing tokenizer 'llama2' with arguments: {'tokenizer_path': 'meta-llama/Llama-2-7b-hf', 'config': {}}
2025-02-11 01:52:20,529 - DEBUG - model.py - Special tokens added to the tokenizer.
2025-02-11 01:52:20,578 - INFO - utils.py - Loaded dataset configuration: {}
2025-02-11 01:52:21,720 - INFO - train.py - Loaded Train Dataset: LIMA Instruct Finetunning Dataset, Dataset Length: 1030 with sub_split_size None.
2025-02-11 01:52:21,720 - INFO - utils.py - Loaded model configuration: {'model_name': 'llama2-base', 'base_model_path': 'meta-llama/Llama-2-7b-hf', 'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}}
2025-02-11 01:52:21,720 - INFO - model.py - Initializing model 'llama2-base' with arguments: {'base_model_path': 'meta-llama/Llama-2-7b-hf', 'config': {'force_download': False, 'device_map': 'cuda:0', 'bnb_config': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': False}}}
2025-02-11 01:52:25,435 - DEBUG - model.py - Base model loaded successfully.
2025-02-11 01:52:42,181 - INFO - utils.py - Loaded trainer configuration: {'r': 64, 'lora_alpha': 64, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'target_modules': ['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj']}
2025-02-11 01:52:43,277 - INFO - utils.py - Loaded trainer configuration: {'resume_from_checkpoint': '/home/hmankodi/instruct_tuning/TrainingLogs/checkpoint-156', 'save_trained_model': True, 'save_strategy': 'epoch', 'save_total_limit': 2, 'run_name': 'llama2_instruct_fulltrain_1', 'report_to': 'tensorboard', 'output_dir': './TrainingLogs/', 'overwrite_output_dir': True, 'max_seq_length': 2048, 'fp16': True, 'do_train': True, 'num_train_epochs': 15, 'per_device_train_batch_size': 20, 'prediction_loss_only': False, 'packing': False, 'logging_strategy': 'steps', 'logging_steps': 1, 'max_grad_norm': 0.3, 'seed': 761340753, 'warmup_ratio': 0.05, 'learning_rate': 1e-05, 'lr_scheduler_type': 'cosine', 'optim': 'paged_adamw_32bit'}
2025-02-11 01:52:43,319 - WARNING - other.py - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-02-11 01:52:43,375 - INFO - train.py - Training started.
