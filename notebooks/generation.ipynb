{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the project directory\n",
    "project_root = os.path.abspath(os.path.join(os.path.join(os.getcwd()), \"..\"))\n",
    "# Add the project root to sys.path\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from lima_dataset import load_lima_dataset, tokenize_text, format_prompt_func, EOT_TOKEN\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_generation_config,\n",
    "    get_generation_samples,\n",
    ")\n",
    "from model import (\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    generate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = read_yaml(\"./configs/generate_config_llama.yaml\")\n",
    "config = read_yaml(\"../configs/generate_config_llama_qlora.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama2',\n",
       " 'meta-llama/Llama-2-7b-hf',\n",
       " {'add_bos_token': True, 'add_eos_token': False})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")\n",
    "tokenizer_name, tokenizer_path, tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'force_download': False,\n",
       " 'device_map': 'cuda:0',\n",
       " 'bnb_config': {'load_in_4bit': True,\n",
       "  'bnb_4bit_quant_type': 'nf4',\n",
       "  'bnb_4bit_compute_dtype': 'float16',\n",
       "  'bnb_4bit_use_double_quant': False},\n",
       " 'use_cache': False,\n",
       " 'pad_token_id': 32000,\n",
       " 'tokenizer_length': 32002}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(\n",
    "    config,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    tokenizer_length=len(tokenizer),\n",
    ")\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f217a704e64b4cb83a7a5082657b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml('../configs/generate_config_llama_qlora.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = get_generation_config(config)\n",
    "generation_config[\"pad_token_id\"] = tokenizer.pad_token_id\n",
    "generation_config[\"eos_token_id\"] = tokenizer.eos_token_id\n",
    "# generation_config['max_new_tokens'] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is reinforcement learning?', 'Explain black hole singularity.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = get_generation_samples(config)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\\n* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\\n* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\\n* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\\n* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\\nHelp me write the paper's introduction.\"\n",
    "# # prompt = \"Plan a day trip in Tokyo. The spots need to be within walking distance to each other.\"\n",
    "# prompt = \"What medicine should I take when I get a cold?\"\n",
    "# # prompt = f\"{prompt}{EOT_TOKEN}\"\n",
    "\n",
    "outs = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    # prompt_samples=prompt,\n",
    "    prompt_samples=samples,\n",
    "    generation_config=generation_config,\n",
    "    use_encode=False,\n",
    "    use_eot_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is reinforcement learning? [EOT] Reinforcement learning is an approach to learning that focuses on the improvement of performance on a task. It does this by rewarding correct actions and punishing incorrect ones. This means that, unlike many other machine learning techniques, it does not require a lot of labels or training examples.\n",
      "\n",
      "Reinforcement learning is typically used in situations where the correct answer is not always known, and where there is no clear way to determine what is correct. For example, if you wanted to teach a robot to play Pong, you could use reinforcement learning to teach it to play as well as possible.\n",
      "\n",
      "Reinforcement learning can be divided into two main categories: supervised and unsupervised. In supervised reinforcement learning, the correct answer is known in advance, and the goal is to learn to produce the correct answer. In unsupervised reinforcement learning, there is no known correct answer, and the goal is to learn to produce the best possible answer.\n",
      "\n",
      "Reinforcement learning is a powerful tool for solving complex problems, and it has been used to solve a wide range of problems, from playing games to driving cars. If you're interested in learning more about reinforcement learning, there are many resources available, including books, articles, and online courses. \n",
      "\n",
      "### Examples of reinforcement learning\n",
      "\n",
      "Here are some examples of how reinforcement learning is used in the real world:\n",
      "\n",
      "* In autonomous cars, reinforcement learning is used to teach the car to drive safely in different environments. The car is rewarded for taking safe actions, and punished for taking unsafe actions.\n",
      "\n",
      "* In games, reinforcement learning is used to teach a computer to play the game. The computer is rewarded for making correct moves, and punished for making incorrect moves.\n",
      "\n",
      "* In robotics, reinforcement learning is used to teach robots to perform tasks. The robot is rewarded for performing the task correctly, and punished for performing the task incorrectly.\n",
      "\n",
      "* In financial markets, reinforcement learning is used to trade stocks and other financial instruments. The trader is rewarded for making profits, and punished for making losses.\n",
      "\n",
      "* In customer service, reinforcement learning is used to teach chatbots to respond to customer queries. The chatbot is rewarded for providing accurate and helpful responses\n"
     ]
    }
   ],
   "source": [
    "print(outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain black hole singularity. [EOT] The singularity of a black hole is an imaginary point, called the event horizon. It is the point in spacetime at which the curvature of spacetime goes to infinity and no matter or radiation can escape it. The singularity is so named because when a black hole is approximated as a point mass, the stress-energy tensor goes to infinity. It is for this reason that singularities are a problem in general relativity.\n",
      "\n",
      "The main reason for the existence of the singularity is that the gravitational force is attractive. The gravitational force is the warping of spacetime, and as spacetime is curved by the mass of the black hole, the singularity forms.\n",
      "\n",
      "It is worth noting that a singularity is not required for a black hole to exist. Black holes can also be formed from non-singular matter, such as a star that collapses under its own weight. The collapse of the matter forms a black hole without a singularity.\n",
      "\n",
      "It is also possible to form a black hole without any singularity at all, if the matter is removed from the centre of the collapsing star before it becomes a black hole. This is known as a naked singularity.\n",
      "\n",
      "There are some theoretical models of the universe which suggest that the singularity will be resolved at the end of the universe, and the singularity will be replaced by a bubble. The idea is that there will be a \"big crunch\", and then the universe will expand again in a \"big bounce\".\n",
      "\n",
      "Here is a video that talks about the black hole singularity in more detail:\n",
      "\n",
      "Here is a link to the video: https://www.youtube.com/watch?v=0sX3iIxFmQw\n",
      "\n",
      "This video is a great resource for anyone interested in learning more about black hole singularities.\n",
      "\n",
      "Here is another video that talks about black hole singularities:\n",
      "\n",
      "Here is a link to the video: https://www.youtube.com/watch?v=vqIH_Mi0Vw8\n",
      "\n",
      "This video is a great resource for anyone interested in learning more about the concept of black hole singularity.\n",
      "\n",
      "And here is a link to an article that talks about black hole singularity:\n",
      "\n",
      "Here is a link to the article: https://www.quora.com/Is\n"
     ]
    }
   ],
   "source": [
    "print(outs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
