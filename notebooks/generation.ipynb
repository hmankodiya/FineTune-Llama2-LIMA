{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the project directory\n",
    "project_root = os.path.abspath(os.path.join(os.path.join(os.getcwd()), \"..\"))\n",
    "# Add the project root to sys.path\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from lima_dataset import load_lima_dataset, tokenize_text, format_prompt_func, EOT_TOKEN\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_generation_config,\n",
    "    get_generation_samples,\n",
    ")\n",
    "from model import (\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    generate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = read_yaml(\"./configs/generate_config_llama.yaml\")\n",
    "config = read_yaml(\"../configs/generate_config_llama_qlora.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama2',\n",
       " 'meta-llama/Llama-2-7b-hf',\n",
       " {'add_bos_token': True, 'add_eos_token': False})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")\n",
    "tokenizer_name, tokenizer_path, tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'force_download': False,\n",
       " 'device_map': 'cuda:0',\n",
       " 'bnb_config': {'load_in_4bit': True,\n",
       "  'bnb_4bit_quant_type': 'nf4',\n",
       "  'bnb_4bit_compute_dtype': 'float16',\n",
       "  'bnb_4bit_use_double_quant': False},\n",
       " 'pad_token_id': 32000,\n",
       " 'tokenizer_length': 32002}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(\n",
    "    config,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    tokenizer_length=len(tokenizer),\n",
    ")\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813ab274f0b24ebc89fdc60cf1660f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml('../configs/generate_config_llama_qlora.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = get_generation_config(config)\n",
    "# generation_config[\"pad_token_id\"] = tokenizer.pad_token_id\n",
    "# generation_config['max_new_tokens'] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = get_generation_samples(config)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# # prompt = \"I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\\n* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\\n* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\\n* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\\n* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\\nHelp me write the paper's introduction.\"\n",
    "# # prompt = \"Plan a day trip in Tokyo. The spots need to be within walking distance to each other.\"\n",
    "# prompt = \"What medicine should I take when I get a cold?\"\n",
    "# # prompt = f\"{prompt}{EOT_TOKEN}\"\n",
    "\n",
    "outs = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_samples=samples,\n",
    "    generation_config=generation_config,\n",
    "    use_encode=False,\n",
    "    eot_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is reinforcement learning?True or false?  avid readers of this book will have noticed that I often refer to reinforcement learning (RL) as the “third pillar” of machine learning. How can that be? After all, machine learning is a very broad term, and we have already covered a lot of ground in the previous chapters. How can we squeeze in one more pillar?  Well, it is true that machine learning is a very broad term. It can be used to refer to supervised learning, unsupervised learning, and reinforcement learning, and in fact, the term machine learning can be applied to all three of these areas. The “third pillar” is a term that I use to refer to reinforcement learning, because it is the least commonly used of the three pillars. However, it is the most powerful of the three. In this chapter, I will introduce you to reinforcement learning, and show you why it is such a powerful technique.  We will start by explaining what reinforcement learning is. Then we will dive into the mathematical foundations of reinforcement learning, and see how we can use reinforcement learning to train agents to perform complex tasks. We will also see how we can use deep neural networks to improve the performance of reinforcement learning agents. Finally, we will see how we can use reinforcement learning to train agents to play Atari games.  In the end, you will be able to understand the power of reinforcement learning, and see how it can be used to train agents to perform complex tasks.\n",
      "## Reinforcement Learning: The Basics\n",
      "In this section, we will introduce you to reinforcement learning. We will start by explaining what reinforcement learning is. Then we will see how we can use reinforcement learning to train agents to perform complex tasks. \n",
      "### What is reinforcement learning?\n",
      "Reinforcement learning is a machine learning technique that is used to train agents to perform complex tasks. The goal of reinforcement learning is to maximize the reward that the agent receives from performing the task. The reward can be any real-valued function that depends on the state of the environment and the action that the agent takes. For example, we can use reinforcement learning to train an agent to play a game of Go. The goal of\n"
     ]
    }
   ],
   "source": [
    "print(outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
