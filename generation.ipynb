{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from lima_dataset import load_lima_dataset, tokenize_text, format_prompt_func, EOT_TOKEN\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_trainer_config,\n",
    "    get_generation_config,\n",
    "    get_generation_samples,\n",
    "    get_lora_config,\n",
    "    _handle_seed,\n",
    "    DEVICE,\n",
    ")\n",
    "from model import (\n",
    "    tokenize_text,\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    load_pretrained_base_llama2_model,\n",
    "    load_lora_model,\n",
    "    generate,\n",
    "    compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = read_yaml(\"./configs/generate_config_llama.yaml\")\n",
    "config = read_yaml(\"./configs/generate_config_llama_lora.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama2', 'meta-llama/Llama-2-7b-hf', {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")\n",
    "tokenizer_name, tokenizer_path, tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'force_download': False, 'device_map': 'cuda:0'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(config)\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['pad_token_id'] = tokenizer.pad_token_id\n",
    "model_config['tokenizer_length'] = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = load_pretrained_base_llama2_model(\n",
    "#     base_model_path, **model_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60688f0d552e46328d8adb5687575e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")\n",
    "# base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32002, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 2048,\n",
       " 'top_p': 0.9,\n",
       " 'temperature': 0.7,\n",
       " 'num_beams': 1,\n",
       " 'top_k': None,\n",
       " 'do_sample': True,\n",
       " 'repetition_penalty': 1.2,\n",
       " 'pad_token_id': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = get_generation_config(config)\n",
    "generation_config['pad_token_id'] = tokenizer.eos_token_id\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\\n* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\\n* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\\n* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\\n* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\\nHelp me write the paper's introduction.\"\n",
    "# prompt = \"Plan a day trip in Tokyo. The spots need to be within walking distance to each other.\"\n",
    "# prompt = \"What medicine should I take when I get a cold?\"\n",
    "# prompt = f\"{prompt}{EOT_TOKEN}\"\n",
    "outs = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_samples=prompt,\n",
    "    generation_config=generation_config,\n",
    "    use_encode=False,\n",
    "    eot_token=None,\n",
    ")\n",
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\n",
      "* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\n",
      "* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\n",
      "* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\n",
      "* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\n",
      "Help me write the paper's introduction. What should it contain? Should we include both these metrics or focus only one of them? How do you compare two models when their performances differ across tasks (language model vs text generation)?  чувак 29 57 +1 / -1\n",
      "\n",
      "The introductory paragraph can be written as follows: A recent work proposed the Exponential Moving Average Transformer (ExeMT) which uses an exponential moving average instead of softmax in the multi-headed self-attention layer. While previous works showed promising results using EMA in various architectures such as recurrent neural networks and convolutional neural networks, there has been little research into applying it to sequence modeling tasks like natural language processing. In contrast to other architectural modifications used in NLP including residual connections, gated units, and bidirectional layers, ExeMAT does not require training from scratch; rather, it requires only small modifications during training time. Our experiments show that ExEMT outperforms standard transformers without loss of accuracy while requiring less memory usage and computational resources compared to those trained from scratch. কুমারী 8 61 জন্যেই সিদার পোতে লগ নিয়ে এক বিশেষ কমেন্ট । হাওয়া সম্প্রতি ম্যানে বেশি ব্যবহার করে নিন । আপনি অনলাইন ভালো, নির্বাচন, ডাউনলোড করতে থাকুন । লিখতে কি বলানো কণ্ঠ? ফাঁপ 21 53 +2 / -2\n",
      "What's your favorite sentence?  It was then that the thought first occurred to him that perhaps he had gone mad himself. As soon as that thought occurs to us, all sorts of strange ideas start growing up inside our minds.  bhk 24 47 +1 / -1\n",
      "Here’s how you could write the abstract section:\n",
      "We propose a novel approach called Exponential Moving Average Transformers (EMA TF). This method replaces the softmax function in the Multi Head Attention block with an exponentially weighted version of it, effectively removing the need for normalization. Experiments demonstrate that EMA TF consistently reduces error rates by >10%, especially under low resource settings, while reducing memory footprint and computation cost. Moreover, EMA TF performs significantly better at capturing context information beyond the current input token relative to existing state-of-the-art models like BERT. Furthermore, preliminary analysis suggests that EMA TF provides improved performance even after fine tuning for downstream classification tasks. Finally, we investigate additional applications of EMA TF in other fields such as computer vision, recommender systems etc., showing promise in many different areas.   tekton 9 56 জন্যেই সিদার পোতে লগ নিয়ে এক বিশেষ কমেন্ট । हाওয়া সম্প্রতি ম্যানে বেশি ব্যবহার করে নিন । আপনি অনলাইন ভালো, নির্বাচন, ডাউনলোড করতে থাকুন । লিখতে কি বলানো কণ্ঠ? ফাঁপ 21 53 +2 / -2\n",
      "You could use either of the following methods to formulate your hypothesis statement.  You could choose one and modify/add based upon suggestions from others here! Also please add your references if applicable:\n",
      "Hypothesis Statement #1:\n",
      "We hypothesize that our proposed Exponential Moving Average Transformers will perform well compared to prior approaches due to its ability to capture longer range dependencies within sequences through the exponential averaging mechanism. Additionally, since EMA TF does not rely on large batch sizes or huge amounts of data for training, it may offer a way forward for developing effective NLP algorithms despite limited availability of annotated datasets. Finally, EMA TF shows potential applicability outside purely linguistic domains such as image recognition and recommendation systems.  kumari_amitabha 11 42 +1 / -1\n",
      "Our experimental results suggest that EMA TF offers several advantages over competitive state-of-the art approaches, specifically in terms of lower latency requirements and higher efficiency in inference times, which are essential given today's demands for realtime response times in complex computing environments. By leveraging advances in technology and software design techniques, future work could explore further enhancements towards achieving optimal performance levels required for practical implementation purposes.  texaswizard 11 42 +1 / -1\n",
      "If you want to consider other metrics besides CoGnaTe and PPL, what would they be? Is there something else worth considering? If so, what would it be?  The most common measures of performance for language modelling tasks are perplexity, BLEU score, and ROUGE scores.\n",
      "For evaluating the quality of generated text, we usually look at things like F1 score and character level BLEU. But we don't often see people evaluate generative models against human judgments of fluency and readability.  sparky 11 42 +1 / -1\n",
      "How did you come up with the name \"Exponential Moving Average Transformer\" (ExTranfor)? Why didn't you just go ahead and say that it's another variant of the regular Transformer?  Since the original transformer only uses a single head for attending to neighbor tokens, we felt that adding multiple heads might help improve the network's overall performance. Thus, we decided to try replacing the softmax function with an exponential moving average (EMA) technique. As shown below, the final output vector for each token consists of contributions from every past hidden unit along with the contribution coming directly from its corresponding linear projection matrix. The output size equals the number of total dimensions plus the summed outputs from every hidden dimension.  sarah 20 39 +2 / -2\n"
     ]
    }
   ],
   "source": [
    "print(outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\n",
      "* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\n",
      "* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\n",
      "* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\n",
      "* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\n",
      "Help me write the paper's introduction.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
