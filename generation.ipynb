{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from lima_dataset import load_lima_dataset, tokenize_text, format_prompt_func, EOT_TOKEN\n",
    "from utils import (\n",
    "    read_yaml,\n",
    "    get_model_config,\n",
    "    get_tokenizer_config,\n",
    "    get_split_config,\n",
    "    get_dataset_config,\n",
    "    get_trainer_config,\n",
    "    get_generation_config,\n",
    "    get_generation_samples,\n",
    "    get_lora_config,\n",
    "    _handle_seed,\n",
    "    DEVICE,\n",
    ")\n",
    "from model import (\n",
    "    tokenize_text,\n",
    "    load_model,\n",
    "    load_tokenizer,\n",
    "    load_pretrained_base_llama2_model,\n",
    "    load_lora_model,\n",
    "    generate,\n",
    "    compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_yaml(\"./configs/generate_config_llama.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama2',\n",
       " 'meta-llama/Llama-2-7b-hf',\n",
       " {'special_token_kwargs': {'pad_token': 'eos_token',\n",
       "   'additional_tokens': ['EOT_TOKEN']}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_name, tokenizer_path, tokenizer_config = get_tokenizer_config(config)\n",
    "tokenizer = load_tokenizer(\n",
    "    tokenizer_name=tokenizer_name,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    tokenizer_config=tokenizer_config,\n",
    ")\n",
    "tokenizer_name, tokenizer_path, tokenizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'force_download': False,\n",
       " 'device_map': 'cuda:0',\n",
       " 'bnb_config': {'load_in_4bit': True,\n",
       "  'bnb_4bit_quant_type': 'nf4',\n",
       "  'bnb_4bit_compute_dtype': 'float16',\n",
       "  'bnb_4bit_use_double_quant': False}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name, model_path, base_model_path, model_config = get_model_config(config)\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['pad_token_id'] = tokenizer.pad_token_id\n",
    "model_config['tokenizer_length'] = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = load_pretrained_base_llama2_model(\n",
    "#     base_model_path, **model_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e213efbe129a456ebb220d86055deea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\n",
    "    model_string=model_name,\n",
    "    model_path=model_path,\n",
    "    base_model_path=base_model_path,\n",
    "    model_config=model_config,\n",
    ")\n",
    "# base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     base_model, \"/home/hmankodi/instruct_tuning/TrainingLogs/checkpoint-468\"\n",
    "# )\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 2048,\n",
       " 'top_p': 0.9,\n",
       " 'temperature': 0.7,\n",
       " 'num_beams': 1,\n",
       " 'top_k': None,\n",
       " 'do_sample': True,\n",
       " 'repetition_penalty': 1.2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = get_generation_config(config)\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\\n* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\\n* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\\n* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\\n* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\\nHelp me write the paper's introduction.\"\n",
    "# prompt = f\"{prompt}{EOT_TOKEN}\"\n",
    "outs = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_samples=prompt,\n",
    "    generation_config=generation_config,\n",
    "    use_encode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm writing a NeurIPS paper about a new model architecture for processing and generating long texts. Here are some facts about the paper:\n",
      "* The main trick is to replace some of the attention heads with an exponential moving average, where the decay rate is learned for each head. We call this architecture ExeMA.\n",
      "* On language modeling, the perplexity difference between our model and a vanilla transformer is negligible, but that's because next-token prediction is almost always a local task, so perplexity won't be sensitive enough to detect any improvements in long-range understanding.\n",
      "* However, on the SCROLLS benchmark, our model improves by 10% over the baseline.\n",
      "* We also have a new metric for measuring coherence in generated text (CoGnaTe), where our model generates text that is 43% more coherent than the baseline.\n",
      "Help me write the paper's introduction.  [EOT] The following paragraph introduces your work, focusing first on why you decided to study this problem (\"we find it interesting\"):\n",
      "We often observe that some models fail when asked to generate or comprehend sentences longer than few dozen tokens. This happens both when training neural networks from scratch as well as during fine tuning existing state-of-the-art methods. As such, we see two trends emerging in literature. Firstly, many papers focus on how to improve pretraining techniques while secondly others investigate how better fine-tuning strategies can help. Inspired by these observations, we ask ourselves whether one could leverage the latter approach directly into an end-to-end learning framework without relying on additional pretrainings steps. To do so, we build upon recent advances towards a self-supervised representation of natural language. Our key insight here is that the key benefits of this type of approaches lie in the ability to train large scale models which generalize beyond their original tasks. Moreover, given sufficient data availability, one could hope to achieve good performance even if no human annotated gold standard exists. Since there exist numerous datasets nowadays containing very long passages, we wondered what would happen if instead of performing a single forward pass through all sequences at once, we move away from global decoding and perform multiple passes over small slices of input data, discarding information across time steps? In other words, will this lead us closer to the solution to the aforementioned problems? To answer this question, we propose a novel multi-headed encoder-decoder model called Exponential Moving Averaging Transformer (ExeMAT). We compare its performance against similar architectures on a variety of downstream NLP applications including machine translation, summarization, image captioning, sentiment analysis, open-ended QA, and cloze tests. Overall, we showcase that our proposed method leads to significant improvements compared to previous works. Specifically, using only public domain datasets, we obtain strong results on most of the above mentioned tasks achieving competitive performance on par with current SOTAs. Furthermore, since our approach does not rely on additional annotations or feature engineering, it may prove useful for future researchers who wish to explore the use of unlabeled data sets containing long passages of text.\n",
      "Then go ahead and explain briefly the experimental setup. Finally, conclude by giving hints regarding potential directions for further investigation/improvements.  atuurally, before providing details on our experiments let us first introduce the ExeMat architecture along with related work. Since the past decade, recurrent sequence-to-sequence models (RNN) based on Long Short Term Memory units (LSTMs) became widely used due to their remarkable success in various Natural Language Processing (NLP) tasks such as Machine Translation, Question Answering or Text Summarisation [Hochreiter et al., 1997] . However, despite early promising results, they quickly showed weaknesses especially concerning longer inputs. Thus, modern deep learning approaches shifted their focus onto Neural Networks (NN)-based architectures like Bidirectional Encoder Representations (BERT) [Devlin et al., 2018b], GPT [Radford et al., 2018a], XLNet [Yang et al., 2019c] , etc. These systems achieved outstanding performances throughout different domains thanks to their capacity to learn dense representations from massive amounts of data. However, they still suffer from several limitations mainly arising from the fact that they ignore long term dependencies between individual tokens. In order to address this shortcoming, previous studies have explored several variants of RNNs trying to either increase their contextual capabilities or exploit external sources of knowledge such as word embeddings or character ngrams [Levy & Goldberg, 1996; Grave et al., 2015; Wan et al., 2019]. While these techniques proved beneficial, they were mostly limited to shorter outputs which required substantial efforts to incorporate them within larger systems. A particularly elegant way to tackle this issue was introduced recently under the name of Scaled Dense Attention (SDA) [Guo et al., 2020a] . It consists of replacing the softmax function typically used for computing inner product scores with the exponential moving average operator (EMA). By doing so, it allows the network to capture the gradients associated with intermediate states rather than just the final output thus leading to improved performance across various settings. Although EMA has been shown effective in solving certain issues encountered by RNNs, its effectiveness remained unclear for sequence generation tasks. For example, Kumar et al. [Kumar et al., 2020] examined the impact of EMA in Transformers and found that contrary to popular belief it did not necessarily result in increased performance. Instead, they observed that depending on the application setting, applying EMA may actually degrade the quality of generated sequences. Given these results, it seems imperative to carefully evaluate the suitability of EMA for the task at hand. To this end, in addition to studying its impact on the overall system’s accuracy, we argue that another relevant measure worth investigating concerns its contribution towards making generated texts less repetitive and grammatically correct. Recently, models able to create fluent and informative narratives received considerable interest among the community [Brown et al., 2020] . They usually combine the power of Deep Learning (DL) algorithms together with specialized linguistic features extracted via statistical learning techniques [Petersen et al., 2017] . Despite their progress, these methods face challenges primarily caused by the high degree of variance present in real world data which makes reliable evaluation difficult. Indeed, although metrics such as BLEU score seem adequately suited to assess abstractive summarisations produced by automatic systems, they might fall short when considering other types of outputs which require an enhanced sense of style and semantic fluency. To overcome this limitation, we devise CoGnAtE - Coherence Generator Assessment Toolbox - an efficient yet flexible toolkit for evaluating sentence coherence measures. It is built around two core components namely a set of highly interpretable heuristics and a suite of metrics designed specifically to analyse continuously evolving documents. With regard to ExeMat itself, prior to elaborating on the advantages brought forth by this innovation, let us take a look at closely related works from the literature. Most of today’s advanced NLP models employ either Recursive Neural Networks (RNNs) or Graphical Models (GM) to process sequentially ordered data. One notable exception however is Distributed Recurrent Neural Networks (DRNN) whose goal lies in combining the strengths of both RNNs and GMs thereby allowing more accurate modelling of distributed representations and complex interactions between variables [Ji et al., 2020] . An alternative to DRNNs is the recent development of Hierarchically Structured Self-Attending Networks (HSAN) [Zhou et al., 2020] which take inspiration from the structure of Human Brain Function Networks (hgbn) comprised of densely interconnected areas that enable higher cognition abilities such as decision making, planning or multilingualism. Likewise, the Transformer model [Vaswani et al., 2017] aims at mimicking the behaviour exhibited by human brains which possess vast connections between neurons. Interestingly, it turns out that HSN can be seen as an extension of SDAN by leveraging hierarchical layers with increasing levels of granularity. Nevertheless, these ideas remain largely theoretical and little empirical evidence shows their practical value in practice. At least until now...  аваiour goal is to develop an efficient yet scalable variant of SRNNs capable of capturing rich long range dependencies. Towards this end, we start off by building up on top of a recent development known as TensorFlow Span Extractor (TFSE) [Xiong et al., 2020] . This module provides state of art token identification mechanisms for bags of words representations commonly adopted by current deep learning frameworks. Following this step, we then apply TFSE on a number of English Wikipedia articles extracted from CommonCrawl  dataset. Afterwards, we project these sequences into a continuous space consisting solely of embedding vectors. Next, inspired by the latest progress made in language modelling [Dos Santos et al., 2019] , we modify the original Trans\n"
     ]
    }
   ],
   "source": [
    "print(outs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
